{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime    import datetime\n",
    "from pathlib import Path\n",
    "from agents.dqn import DQNAgent\n",
    "from agents.dueling_dqn import DuelingDQNAgent\n",
    "\n",
    "# hot reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# CartPole-specific configuration\n",
    "CONFIG = {\n",
    "    \"env_name\": \"CartPole-v1\",\n",
    "    \"num_episodes\": 1000,\n",
    "    \"save_every_n\": 50,\n",
    "    \n",
    "    # Agent settings\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"use_cnn\": False,  # CartPole uses MLP\n",
    "    \n",
    "    # DQN specific\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"gamma\": 0.99,\n",
    "    \"buffer_size\": 50000,  \n",
    "    \"batch_size\": 32,\n",
    "    \"target_update_freq\": 2, # (in steps), set to 1 for soft update\n",
    "    \"tau\": 0.001,\n",
    "    \"eps_start\": 1.0,\n",
    "    \"eps_end\": 0.01,\n",
    "    \"eps_decay\": 0.99,\n",
    "    \"hidden_dims\": [32,32],  \n",
    "    \"gradient_clip\": 1.0,\n",
    "    \"double_dqn\": True,\n",
    "    \"update_freq\": 4,\n",
    "    \n",
    "    \"per_alpha\": 0.6,        # How much prioritization to use (0 = uniform, 1 = full prioritization)\n",
    "    \"per_beta_start\": 0.4,   # Initial importance sampling correction\n",
    "    \"per_beta_end\": 1.0,     # Final importance sampling correction\n",
    "    \"per_beta_steps\": 100000, # Steps over which to anneal beta\n",
    "\n",
    "    \"mlp_feature_extractor_hidden_dims\": [64],\n",
    "}\n",
    "\n",
    "def plot_training_history(returns, q_losses, q_values, save_dir):\n",
    "    \"\"\"Plot and save training metrics.\"\"\"\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 15))\n",
    "    \n",
    "    # Plot returns\n",
    "    ax1.plot(returns)\n",
    "    ax1.set_title('Episode Returns')\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Return')\n",
    "    \n",
    "    # Plot Q-losses\n",
    "    ax2.plot(q_losses)\n",
    "    ax2.set_title('Q-Loss')\n",
    "    ax2.set_xlabel('Episode')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    \n",
    "    # Plot average Q-values\n",
    "    ax3.plot(q_values)\n",
    "    ax3.set_title('Average Q-Value')\n",
    "    ax3.set_xlabel('Episode')\n",
    "    ax3.set_ylabel('Q-Value')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_dir / 'training_curves.png')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Agent architecture:\n",
      "DuelingQNetwork(\n",
      "  (feature_extractor): MLPBackbone(\n",
      "    (model): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (advantage_network): MLPBackbone(\n",
      "    (model): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=32, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=32, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (value_network): MLPBackbone(\n",
      "    (model): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=32, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=32, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Episode 10/1000\n",
      "Average Return (last 10): 22.90\n",
      "Epsilon: 0.904\n",
      "Steps Taken: 17\n",
      "Latest Q-Loss: 0.004038\n",
      "--------------------\n",
      "Episode 20/1000\n",
      "Average Return (last 10): 21.30\n",
      "Epsilon: 0.818\n",
      "Steps Taken: 19\n",
      "Latest Q-Loss: 0.000980\n",
      "--------------------\n",
      "Episode 30/1000\n",
      "Average Return (last 10): 20.70\n",
      "Epsilon: 0.740\n",
      "Steps Taken: 12\n",
      "Latest Q-Loss: 0.000408\n",
      "--------------------\n",
      "Episode 40/1000\n",
      "Average Return (last 10): 16.40\n",
      "Epsilon: 0.669\n",
      "Steps Taken: 14\n",
      "Latest Q-Loss: 0.000374\n",
      "--------------------\n",
      "Episode 50/1000\n",
      "Average Return (last 10): 19.10\n",
      "Epsilon: 0.605\n",
      "Steps Taken: 28\n",
      "Latest Q-Loss: 0.000284\n",
      "--------------------\n",
      "Episode 60/1000\n",
      "Average Return (last 10): 15.50\n",
      "Epsilon: 0.547\n",
      "Steps Taken: 18\n",
      "Latest Q-Loss: 0.000410\n",
      "--------------------\n",
      "Episode 70/1000\n",
      "Average Return (last 10): 13.60\n",
      "Epsilon: 0.495\n",
      "Steps Taken: 26\n",
      "Latest Q-Loss: 0.000688\n",
      "--------------------\n",
      "Episode 80/1000\n",
      "Average Return (last 10): 17.20\n",
      "Epsilon: 0.448\n",
      "Steps Taken: 11\n",
      "Latest Q-Loss: 0.000965\n",
      "--------------------\n",
      "Episode 90/1000\n",
      "Average Return (last 10): 12.40\n",
      "Epsilon: 0.405\n",
      "Steps Taken: 11\n",
      "Latest Q-Loss: 0.002121\n",
      "--------------------\n",
      "Episode 100/1000\n",
      "Average Return (last 10): 12.30\n",
      "Epsilon: 0.366\n",
      "Steps Taken: 12\n",
      "Latest Q-Loss: 0.000588\n",
      "--------------------\n",
      "Episode 110/1000\n",
      "Average Return (last 10): 10.80\n",
      "Epsilon: 0.331\n",
      "Steps Taken: 9\n",
      "Latest Q-Loss: 0.001503\n",
      "--------------------\n",
      "Episode 120/1000\n",
      "Average Return (last 10): 13.70\n",
      "Epsilon: 0.299\n",
      "Steps Taken: 11\n",
      "Latest Q-Loss: 0.001408\n",
      "--------------------\n",
      "Episode 130/1000\n",
      "Average Return (last 10): 11.50\n",
      "Epsilon: 0.271\n",
      "Steps Taken: 9\n",
      "Latest Q-Loss: 0.001623\n",
      "--------------------\n",
      "Episode 140/1000\n",
      "Average Return (last 10): 10.20\n",
      "Epsilon: 0.245\n",
      "Steps Taken: 8\n",
      "Latest Q-Loss: 0.003091\n",
      "--------------------\n",
      "Episode 150/1000\n",
      "Average Return (last 10): 10.50\n",
      "Epsilon: 0.221\n",
      "Steps Taken: 10\n",
      "Latest Q-Loss: 0.002901\n",
      "--------------------\n",
      "Episode 160/1000\n",
      "Average Return (last 10): 11.70\n",
      "Epsilon: 0.200\n",
      "Steps Taken: 8\n",
      "Latest Q-Loss: 0.002641\n",
      "--------------------\n",
      "Episode 170/1000\n",
      "Average Return (last 10): 9.70\n",
      "Epsilon: 0.181\n",
      "Steps Taken: 10\n",
      "Latest Q-Loss: 0.006058\n",
      "--------------------\n",
      "Episode 180/1000\n",
      "Average Return (last 10): 10.20\n",
      "Epsilon: 0.164\n",
      "Steps Taken: 10\n",
      "Latest Q-Loss: 0.002235\n",
      "--------------------\n",
      "Episode 190/1000\n",
      "Average Return (last 10): 10.00\n",
      "Epsilon: 0.148\n",
      "Steps Taken: 9\n",
      "Latest Q-Loss: 0.004447\n",
      "--------------------\n",
      "Episode 200/1000\n",
      "Average Return (last 10): 10.70\n",
      "Epsilon: 0.134\n",
      "Steps Taken: 9\n",
      "Latest Q-Loss: 0.003453\n",
      "--------------------\n",
      "Episode 210/1000\n",
      "Average Return (last 10): 9.50\n",
      "Epsilon: 0.121\n",
      "Steps Taken: 9\n",
      "Latest Q-Loss: 0.004706\n",
      "--------------------\n",
      "Episode 220/1000\n",
      "Average Return (last 10): 10.50\n",
      "Epsilon: 0.110\n",
      "Steps Taken: 8\n",
      "Latest Q-Loss: 0.004838\n",
      "--------------------\n",
      "Episode 230/1000\n",
      "Average Return (last 10): 10.20\n",
      "Epsilon: 0.099\n",
      "Steps Taken: 10\n",
      "Latest Q-Loss: 0.005305\n",
      "--------------------\n",
      "Episode 240/1000\n",
      "Average Return (last 10): 9.50\n",
      "Epsilon: 0.090\n",
      "Steps Taken: 9\n",
      "Latest Q-Loss: 0.004236\n",
      "--------------------\n",
      "Episode 250/1000\n",
      "Average Return (last 10): 9.80\n",
      "Epsilon: 0.081\n",
      "Steps Taken: 12\n",
      "Latest Q-Loss: 0.005434\n",
      "--------------------\n",
      "Episode 260/1000\n",
      "Average Return (last 10): 11.10\n",
      "Epsilon: 0.073\n",
      "Steps Taken: 9\n",
      "Latest Q-Loss: 0.008177\n",
      "--------------------\n",
      "Episode 270/1000\n",
      "Average Return (last 10): 11.60\n",
      "Epsilon: 0.066\n",
      "Steps Taken: 9\n",
      "Latest Q-Loss: 0.005017\n",
      "--------------------\n",
      "Episode 280/1000\n",
      "Average Return (last 10): 9.20\n",
      "Epsilon: 0.060\n",
      "Steps Taken: 8\n",
      "Latest Q-Loss: 0.007664\n",
      "--------------------\n",
      "Episode 290/1000\n",
      "Average Return (last 10): 11.30\n",
      "Epsilon: 0.054\n",
      "Steps Taken: 9\n",
      "Latest Q-Loss: 0.005920\n",
      "--------------------\n",
      "Episode 300/1000\n",
      "Average Return (last 10): 9.60\n",
      "Epsilon: 0.049\n",
      "Steps Taken: 10\n",
      "Latest Q-Loss: 0.004581\n",
      "--------------------\n",
      "Episode 310/1000\n",
      "Average Return (last 10): 13.80\n",
      "Epsilon: 0.044\n",
      "Steps Taken: 9\n",
      "Latest Q-Loss: 0.007510\n",
      "--------------------\n",
      "Episode 320/1000\n",
      "Average Return (last 10): 9.50\n",
      "Epsilon: 0.040\n",
      "Steps Taken: 10\n",
      "Latest Q-Loss: 0.010534\n",
      "--------------------\n",
      "Episode 330/1000\n",
      "Average Return (last 10): 12.10\n",
      "Epsilon: 0.036\n",
      "Steps Taken: 11\n",
      "Latest Q-Loss: 0.012447\n",
      "--------------------\n",
      "Episode 340/1000\n",
      "Average Return (last 10): 24.20\n",
      "Epsilon: 0.033\n",
      "Steps Taken: 9\n",
      "Latest Q-Loss: 0.007549\n",
      "--------------------\n",
      "Episode 350/1000\n",
      "Average Return (last 10): 22.00\n",
      "Epsilon: 0.030\n",
      "Steps Taken: 12\n",
      "Latest Q-Loss: 0.008046\n",
      "--------------------\n",
      "Episode 360/1000\n",
      "Average Return (last 10): 28.00\n",
      "Epsilon: 0.027\n",
      "Steps Taken: 10\n",
      "Latest Q-Loss: 0.006025\n",
      "--------------------\n",
      "Episode 370/1000\n",
      "Average Return (last 10): 15.40\n",
      "Epsilon: 0.024\n",
      "Steps Taken: 10\n",
      "Latest Q-Loss: 0.010591\n",
      "--------------------\n",
      "Episode 380/1000\n",
      "Average Return (last 10): 21.80\n",
      "Epsilon: 0.022\n",
      "Steps Taken: 9\n",
      "Latest Q-Loss: 0.009065\n",
      "--------------------\n",
      "Episode 390/1000\n",
      "Average Return (last 10): 20.20\n",
      "Epsilon: 0.020\n",
      "Steps Taken: 115\n",
      "Latest Q-Loss: 0.011130\n",
      "--------------------\n",
      "Episode 400/1000\n",
      "Average Return (last 10): 30.70\n",
      "Epsilon: 0.018\n",
      "Steps Taken: 12\n",
      "Latest Q-Loss: 0.013502\n",
      "--------------------\n",
      "Episode 410/1000\n",
      "Average Return (last 10): 41.20\n",
      "Epsilon: 0.016\n",
      "Steps Taken: 11\n",
      "Latest Q-Loss: 0.023473\n",
      "--------------------\n",
      "Episode 420/1000\n",
      "Average Return (last 10): 14.30\n",
      "Epsilon: 0.015\n",
      "Steps Taken: 11\n",
      "Latest Q-Loss: 0.012581\n",
      "--------------------\n",
      "Episode 430/1000\n",
      "Average Return (last 10): 10.80\n",
      "Epsilon: 0.013\n",
      "Steps Taken: 10\n",
      "Latest Q-Loss: 0.012087\n",
      "--------------------\n",
      "Episode 440/1000\n",
      "Average Return (last 10): 19.20\n",
      "Epsilon: 0.012\n",
      "Steps Taken: 109\n",
      "Latest Q-Loss: 0.014999\n",
      "--------------------\n",
      "Episode 450/1000\n",
      "Average Return (last 10): 47.40\n",
      "Epsilon: 0.011\n",
      "Steps Taken: 9\n",
      "Latest Q-Loss: 0.013391\n",
      "--------------------\n",
      "Episode 460/1000\n",
      "Average Return (last 10): 11.40\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 10\n",
      "Latest Q-Loss: 0.022517\n",
      "--------------------\n",
      "Episode 470/1000\n",
      "Average Return (last 10): 14.40\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 9\n",
      "Latest Q-Loss: 0.013163\n",
      "--------------------\n",
      "Episode 480/1000\n",
      "Average Return (last 10): 65.60\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 111\n",
      "Latest Q-Loss: 0.018017\n",
      "--------------------\n",
      "Episode 490/1000\n",
      "Average Return (last 10): 153.50\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 153\n",
      "Latest Q-Loss: 0.021490\n",
      "--------------------\n",
      "Episode 500/1000\n",
      "Average Return (last 10): 186.50\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 392\n",
      "Latest Q-Loss: 0.019687\n",
      "--------------------\n",
      "Episode 510/1000\n",
      "Average Return (last 10): 234.60\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 217\n",
      "Latest Q-Loss: 0.020305\n",
      "--------------------\n",
      "Episode 520/1000\n",
      "Average Return (last 10): 292.70\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 240\n",
      "Latest Q-Loss: 0.038935\n",
      "--------------------\n",
      "Episode 530/1000\n",
      "Average Return (last 10): 246.50\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 496\n",
      "Latest Q-Loss: 0.043007\n",
      "--------------------\n",
      "Episode 540/1000\n",
      "Average Return (last 10): 216.90\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 208\n",
      "Latest Q-Loss: 0.069821\n",
      "--------------------\n",
      "Episode 550/1000\n",
      "Average Return (last 10): 289.70\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 277\n",
      "Latest Q-Loss: 0.065632\n",
      "--------------------\n",
      "Episode 560/1000\n",
      "Average Return (last 10): 329.50\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 302\n",
      "Latest Q-Loss: 0.082029\n",
      "--------------------\n",
      "Episode 570/1000\n",
      "Average Return (last 10): 307.20\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 454\n",
      "Latest Q-Loss: 0.077389\n",
      "--------------------\n",
      "Episode 580/1000\n",
      "Average Return (last 10): 244.60\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 364\n",
      "Latest Q-Loss: 0.115907\n",
      "--------------------\n",
      "Episode 590/1000\n",
      "Average Return (last 10): 272.30\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 500\n",
      "Latest Q-Loss: 0.110471\n",
      "--------------------\n",
      "Episode 600/1000\n",
      "Average Return (last 10): 220.00\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 103\n",
      "Latest Q-Loss: 0.138058\n",
      "--------------------\n",
      "Episode 610/1000\n",
      "Average Return (last 10): 202.90\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 139\n",
      "Latest Q-Loss: 0.146964\n",
      "--------------------\n",
      "Episode 620/1000\n",
      "Average Return (last 10): 194.70\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 190\n",
      "Latest Q-Loss: 0.095861\n",
      "--------------------\n",
      "Episode 630/1000\n",
      "Average Return (last 10): 207.10\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 216\n",
      "Latest Q-Loss: 0.136901\n",
      "--------------------\n",
      "Episode 640/1000\n",
      "Average Return (last 10): 190.90\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 95\n",
      "Latest Q-Loss: 0.110624\n",
      "--------------------\n",
      "Episode 650/1000\n",
      "Average Return (last 10): 189.50\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 190\n",
      "Latest Q-Loss: 0.136856\n",
      "--------------------\n",
      "Episode 660/1000\n",
      "Average Return (last 10): 191.00\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 108\n",
      "Latest Q-Loss: 0.085159\n",
      "--------------------\n",
      "Episode 670/1000\n",
      "Average Return (last 10): 171.70\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 103\n",
      "Latest Q-Loss: 0.091707\n",
      "--------------------\n",
      "Episode 680/1000\n",
      "Average Return (last 10): 303.50\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 500\n",
      "Latest Q-Loss: 0.160921\n",
      "--------------------\n",
      "Episode 690/1000\n",
      "Average Return (last 10): 291.30\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 229\n",
      "Latest Q-Loss: 0.204610\n",
      "--------------------\n",
      "Episode 700/1000\n",
      "Average Return (last 10): 253.50\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 155\n",
      "Latest Q-Loss: 0.112475\n",
      "--------------------\n",
      "Episode 710/1000\n",
      "Average Return (last 10): 224.90\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 200\n",
      "Latest Q-Loss: 0.211203\n",
      "--------------------\n",
      "Episode 720/1000\n",
      "Average Return (last 10): 213.70\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 135\n",
      "Latest Q-Loss: 0.393773\n",
      "--------------------\n",
      "Episode 730/1000\n",
      "Average Return (last 10): 167.80\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 500\n",
      "Latest Q-Loss: 0.367281\n",
      "--------------------\n",
      "Episode 740/1000\n",
      "Average Return (last 10): 312.30\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 195\n",
      "Latest Q-Loss: 0.545400\n",
      "--------------------\n",
      "Episode 750/1000\n",
      "Average Return (last 10): 260.50\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 221\n",
      "Latest Q-Loss: 0.412781\n",
      "--------------------\n",
      "Episode 760/1000\n",
      "Average Return (last 10): 273.30\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 100\n",
      "Latest Q-Loss: 0.738139\n",
      "--------------------\n",
      "Episode 770/1000\n",
      "Average Return (last 10): 300.60\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 227\n",
      "Latest Q-Loss: 0.646345\n",
      "--------------------\n",
      "Episode 780/1000\n",
      "Average Return (last 10): 186.50\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 134\n",
      "Latest Q-Loss: 1.764197\n",
      "--------------------\n",
      "Episode 790/1000\n",
      "Average Return (last 10): 107.70\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 33\n",
      "Latest Q-Loss: 0.858270\n",
      "--------------------\n",
      "Episode 800/1000\n",
      "Average Return (last 10): 132.30\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 460\n",
      "Latest Q-Loss: 0.947461\n",
      "--------------------\n",
      "Episode 810/1000\n",
      "Average Return (last 10): 213.60\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 242\n",
      "Latest Q-Loss: 0.970419\n",
      "--------------------\n",
      "Episode 820/1000\n",
      "Average Return (last 10): 267.70\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 500\n",
      "Latest Q-Loss: 1.266472\n",
      "--------------------\n",
      "Episode 830/1000\n",
      "Average Return (last 10): 226.90\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 500\n",
      "Latest Q-Loss: 0.927683\n",
      "--------------------\n",
      "Episode 840/1000\n",
      "Average Return (last 10): 251.40\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 500\n",
      "Latest Q-Loss: 1.133511\n",
      "--------------------\n",
      "Episode 850/1000\n",
      "Average Return (last 10): 285.20\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 312\n",
      "Latest Q-Loss: 1.318266\n",
      "--------------------\n",
      "Episode 860/1000\n",
      "Average Return (last 10): 251.20\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 128\n",
      "Latest Q-Loss: 1.617705\n",
      "--------------------\n",
      "Episode 870/1000\n",
      "Average Return (last 10): 163.60\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 486\n",
      "Latest Q-Loss: 1.331546\n",
      "--------------------\n",
      "Episode 880/1000\n",
      "Average Return (last 10): 212.50\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 114\n",
      "Latest Q-Loss: 1.069583\n",
      "--------------------\n",
      "Episode 890/1000\n",
      "Average Return (last 10): 160.30\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 120\n",
      "Latest Q-Loss: 1.352588\n",
      "--------------------\n",
      "Episode 900/1000\n",
      "Average Return (last 10): 115.00\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 120\n",
      "Latest Q-Loss: 1.244653\n",
      "--------------------\n",
      "Episode 910/1000\n",
      "Average Return (last 10): 142.90\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 236\n",
      "Latest Q-Loss: 1.273821\n",
      "--------------------\n",
      "Episode 920/1000\n",
      "Average Return (last 10): 142.40\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 149\n",
      "Latest Q-Loss: 1.098361\n",
      "--------------------\n",
      "Episode 930/1000\n",
      "Average Return (last 10): 125.20\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 108\n",
      "Latest Q-Loss: 1.188406\n",
      "--------------------\n",
      "Episode 940/1000\n",
      "Average Return (last 10): 123.10\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 108\n",
      "Latest Q-Loss: 1.166417\n",
      "--------------------\n",
      "Episode 950/1000\n",
      "Average Return (last 10): 116.70\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 100\n",
      "Latest Q-Loss: 1.751713\n",
      "--------------------\n",
      "Episode 960/1000\n",
      "Average Return (last 10): 131.80\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 95\n",
      "Latest Q-Loss: 2.224871\n",
      "--------------------\n",
      "Episode 970/1000\n",
      "Average Return (last 10): 105.60\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 109\n",
      "Latest Q-Loss: 1.544126\n",
      "--------------------\n",
      "Episode 980/1000\n",
      "Average Return (last 10): 116.90\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 9\n",
      "Latest Q-Loss: 0.794797\n",
      "--------------------\n",
      "Episode 990/1000\n",
      "Average Return (last 10): 116.40\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 94\n",
      "Latest Q-Loss: 1.769630\n",
      "--------------------\n",
      "Episode 1000/1000\n",
      "Average Return (last 10): 120.30\n",
      "Epsilon: 0.010\n",
      "Steps Taken: 134\n",
      "Latest Q-Loss: 1.451889\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    # Create environment\n",
    "    env = gym.make(CONFIG[\"env_name\"])\n",
    "    \n",
    "    # Initialize agent\n",
    "    agent = DuelingDQNAgent(\n",
    "        env=env,\n",
    "        device=CONFIG[\"device\"],\n",
    "        use_cnn=CONFIG[\"use_cnn\"],\n",
    "        lr=CONFIG[\"learning_rate\"],\n",
    "        gamma=CONFIG[\"gamma\"],\n",
    "        buffer_size=CONFIG[\"buffer_size\"],\n",
    "        batch_size=CONFIG[\"batch_size\"],\n",
    "        target_update_freq=CONFIG[\"target_update_freq\"],\n",
    "        tau=CONFIG[\"tau\"],\n",
    "        eps_start=CONFIG[\"eps_start\"],\n",
    "        eps_end=CONFIG[\"eps_end\"],\n",
    "        eps_decay=CONFIG[\"eps_decay\"],\n",
    "        hidden_dims=CONFIG[\"hidden_dims\"],\n",
    "        gradient_clip=CONFIG[\"gradient_clip\"],\n",
    "        double_dqn=CONFIG[\"double_dqn\"],\n",
    "        update_freq=CONFIG[\"update_freq\"],\n",
    "        per_alpha=CONFIG[\"per_alpha\"],\n",
    "        per_beta_start=CONFIG[\"per_beta_start\"],\n",
    "        per_beta_end=CONFIG[\"per_beta_end\"],\n",
    "        per_beta_steps=CONFIG[\"per_beta_steps\"],\n",
    "        mlp_feature_extractor_hidden_dims=CONFIG[\"mlp_feature_extractor_hidden_dims\"]\n",
    "    )\n",
    "    \n",
    "    # Create experiment directory\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    exp_dir = Path(f\"./experiments/CartPole_{timestamp}\")\n",
    "    exp_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Training loop\n",
    "    returns = []\n",
    "    q_losses = []\n",
    "    q_values = []\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    print(f\"Agent architecture:\\n{agent.q_network}\")\n",
    "    \n",
    "    for episode in range(CONFIG[\"num_episodes\"]):\n",
    "        results = agent.run_episode(env)\n",
    "        \n",
    "        returns.append(results[\"total_return\"])\n",
    "        q_losses.append(results[\"q_loss\"])\n",
    "        q_values.append(results[\"mean_q_value\"])\n",
    "        \n",
    "        # Print progress\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            avg_return = np.mean(returns[-10:])\n",
    "            print(f\"Episode {episode + 1}/{CONFIG['num_episodes']}\")\n",
    "            print(f\"Average Return (last 10): {avg_return:.2f}\")\n",
    "            print(f\"Epsilon: {agent.eps:.3f}\")\n",
    "            print(f\"Steps Taken: {results['steps']}\")\n",
    "            print(f\"Latest Q-Loss: {results['q_loss']:.6f}\")\n",
    "            print(\"--------------------\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if (episode + 1) % CONFIG[\"save_every_n\"] == 0:\n",
    "            checkpoint_path = exp_dir / f\"checkpoint_episode_{episode+1}.pth\"\n",
    "            agent.save(checkpoint_path)\n",
    "            \n",
    "            # Plot current progress\n",
    "            plot_training_history(returns, q_losses, q_values, exp_dir)\n",
    "    \n",
    "    # Final plots\n",
    "    plot_training_history(returns, q_losses, q_values, exp_dir)\n",
    "    \n",
    "    return agent, env, returns\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    agent, env, returns = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slitherl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
